{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6gICSGYpnJyI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import re\n",
    "import xlsxwriter\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tDDcGUD6nJyV"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6LZrxF85nJyY"
   },
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "akthzbfhnJya"
   },
   "outputs": [],
   "source": [
    "def writeln(errors, filename):\n",
    "    workbook = xlsxwriter.Workbook(filename)\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    #Fill in the column names\n",
    "    for x in range(len(errors[0])):\n",
    "        worksheet.write(0, x, errors[0][x])\n",
    "\n",
    "    col = 0\n",
    "    for row_num in range(1, len(errors)):\n",
    "        error = errors[row_num]\n",
    "        for x in range(len(error)):\n",
    "            worksheet.write(row_num, col + x, error[x])\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kBOmgczUnJyc"
   },
   "outputs": [],
   "source": [
    "def find_subj(pred):\n",
    "    # print(pred, [child.text for child in pred.lefts]) use pred.lefts to find quotes\n",
    "    # simple cases\n",
    "    subjects = []\n",
    "    subjects = [child for child in list(pred.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "\n",
    "    # handling 'there is' and 'there are' cases\n",
    "    if 'there' in list(i.text.lower() for i in pred.children):\n",
    "        subjects += [child for child in list(pred.children) if child.dep_ == 'attr']\n",
    "\n",
    "    # if predicate is an auxiliary, we want to take subjects of its head\n",
    "    if pred.dep_.startswith('aux'):\n",
    "        subjects += [child for child in list(pred.head.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "    \n",
    "    # handling conjuncts: multiple subjects as in 'Mother and father are key figures in a child's life'.\n",
    "    add_subj = []\n",
    "    for subject in subjects:\n",
    "        add_subj += list(subject.conjuncts)\n",
    "\n",
    "    cur_pred = pred\n",
    "    while len(subjects) == 0 and cur_pred.dep_ == \"conj\":\n",
    "        cur_pred = cur_pred.head\n",
    "        subjects = find_subj(cur_pred)\n",
    "\n",
    "    subjects += add_subj\n",
    "\n",
    "    # the subjects' order may be different from sentence order, so we arrange it right\n",
    "    subjects.sort(key=lambda subj: subj.i)\n",
    "\n",
    "    if len(subjects) == 1 and subjects[0].text.lower() in ['who', 'that', 'which'] and pred.dep_ == 'relcl':\n",
    "        subjects = [pred.head]\n",
    "\n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "x4mj0yyanJyg"
   },
   "outputs": [],
   "source": [
    "def find_pred_subj(doc):\n",
    "    pred_sub = list()\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['AUX', 'VERB']:\n",
    "            # negation: cases like \"He doesn't scare me\"   \n",
    "            aux = None\n",
    "            if [ch for ch in list(token.lefts) if ch.dep_ == 'neg']:\n",
    "                children = list(token.children)\n",
    "                for ch in children:\n",
    "                    if ch.dep_ == 'aux' and not aux:\n",
    "                        aux = ch\n",
    "                if aux:\n",
    "                    pred_sub += [(aux, find_subj(aux))]\n",
    "\n",
    "\n",
    "            # if the predicate is analytical like 'I have done',\n",
    "            # spacy rightfully considers the participle to be the root,\n",
    "            # but we need grammatical info, so we will consider aux the root\n",
    "            if not aux: # for when negation is expressed with 'never' etc and does not need aux support\n",
    "                if token.tag_ in ['VBN', 'VBG']:\n",
    "                    aux = None\n",
    "                    children = list(token.children)\n",
    "                    for ch in children:\n",
    "                        if ch.dep_ == 'aux' and ch.pos_ in ['VERB', 'AUX'] and ch.tag_ != 'VBN':\n",
    "                            aux = ch\n",
    "                        elif not aux and ch.dep_ == 'auxpass' and ch.pos_ in ['VERB', 'AUX']:\n",
    "                            aux = ch\n",
    "                    if aux:\n",
    "                        pred_sub += [(aux, find_subj(aux))]\n",
    "\n",
    "                # all other cases\n",
    "                elif token.dep_ in ['ROOT', 'ccomp', 'xcomp', 'acl', 'relcl']:\n",
    "                    pred_sub += [(token, find_subj(token))]\n",
    "\n",
    "                # conjuncts: when there are multiple predicates connected by conjunction\n",
    "                elif token.dep_ == 'conj' and token.head.dep_ in ['ROOT', 'ccomp', 'xcomp', 'acl', 'relcl']:\n",
    "                    pred_sub += [(token, find_subj(token))]\n",
    "                \n",
    "    return pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OIP9NwitnJyl"
   },
   "outputs": [],
   "source": [
    "def errors(ps):\n",
    "    res = []\n",
    "    for pair in ps:\n",
    "        subj_agr, pred_agr = None, None #what each must agree with, variables must coincide in the end\n",
    "        pred = pair[0]\n",
    "        subj = pair[1]\n",
    "        if len(subj) == 1:\n",
    "            subject = subj[0]\n",
    "            s = subject.text.lower()\n",
    "            subject_left_children = subject.lefts\n",
    "            subject_is_numeral = False\n",
    "            for child in subject_left_children:\n",
    "                if child.pos_ == 'NUM':\n",
    "                    subject_is_numeral = True\n",
    "                    break\n",
    "            if subject_is_numeral:\n",
    "                continue\n",
    "            if s not in ambiguous:\n",
    "                children = list(ch for ch in subject.children)\n",
    "                children_text = list(ch.text.lower() for ch in children)\n",
    "                \n",
    "                #singular only pronouns\n",
    "                if s in sing_only or subject.ent_type_ == 'ORG':\n",
    "                    subj_agr = 'sg'\n",
    "                elif subject.tag_ == 'VB':\n",
    "                    subj_agr = 'sg'\n",
    "                    \n",
    "                # either singular or plural pronouns\n",
    "                # if they have an 'of N, N, N...', after them we will require check if verb agrees with the last noun\n",
    "                elif (s in {'some', 'any', 'none', 'all', 'most'} \\\n",
    "                      and 'of' in children_text):\n",
    "                    of = [ch for ch in children if ch.text.lower() == 'of'][0]\n",
    "                    noun = [ch for ch in of.children if ch.pos_ == 'NOUN']\n",
    "                    if noun:\n",
    "                        noun = noun[0]\n",
    "                        while [ch for ch in noun.children if (ch.dep_ == 'conj' and ch.pos_ == 'NOUN')]:\n",
    "                            noun = [ch for ch in noun.children if (ch.dep_ == 'conj' and ch.pos_ == 'NOUN')][-1]\n",
    "                        if noun.tag_ in ['NNS', 'NNPS'] or noun.text.lower() in plur_only:\n",
    "                            subj_agr = 'pl'\n",
    "                        elif noun.tag_ in ['NN', 'NNP'] or noun.text.lower() in sing_only:\n",
    "                            subj_agr = 'sg'\n",
    "                    \n",
    "                # plural only pronouns\n",
    "                elif s in plur_only and not children:\n",
    "                    subj_agr = 'pl'\n",
    "                    \n",
    "                elif s in {'i', 'we', 'you', 'they'}:\n",
    "                    subj_agr = 'pl'\n",
    "                elif s in {'he', 'she', 'it'}:\n",
    "                    subj_agr = 'sg'\n",
    "                    \n",
    "                elif s == 'number':\n",
    "                    if 'a' in children_text and 'of' in children_text:\n",
    "                        subj_agr = 'pl'\n",
    "                    else: subj_agr = 'sg'\n",
    "                    \n",
    "                # predicates in non-head clauses with 'who', 'that' agree with noun in head clause\n",
    "                elif s in ['who', 'that', 'which']:\n",
    "                    if pred.dep_ == 'relcl': \n",
    "                        print(\"This is probably a bug!\")\n",
    "                        #why relcl? we can only be sure about this tag that it is the case we're looking for.\n",
    "                        #other possible predicate tags include '...comp', but these also apply\n",
    "                        #in cases like \"I asked the boys who was the winner\",\n",
    "                        #which, although with incorrect word order,\n",
    "                        #are still clearly present in Russian essays\n",
    "                        #and will be parsed by spacy as 'ccomp'\n",
    "                        head = pred.head\n",
    "                        if not head.conjuncts:\n",
    "                            if head.tag_ in ['NNS', 'NNPS'] or head.text.lower() in plur_only:\n",
    "                                subj_agr = 'pl'\n",
    "                            elif head.tag_ in ['NN', 'NNP'] or head.text.lower() in sing_only:\n",
    "                                subj_agr = 'sg'\n",
    "                        else:\n",
    "                            conjuncts = list(head.conjuncts)+[head]\n",
    "                            for conjunct in conjuncts:\n",
    "                                if 'and' in list(child.text.lower() for child in conjunct.children):\n",
    "                                    subj_agr = 'pl' \n",
    "                        print(subj_agr)\n",
    "                elif subject.tag_ in ['NNS', 'NNPS']:\n",
    "                    subj_agr = 'pl'\n",
    "                elif subject.tag_ in ['NN', 'NNP', 'VBG']:\n",
    "                    subj_agr = 'sg'\n",
    "                if subject.ent_type_ == 'LOC' or subject.ent_type_ == 'GPE':\n",
    "                    if subject.tag_ in ['NNS', 'NNPS']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        subj_arg = 'sg'\n",
    "                    \n",
    "        elif len(subj) > 1:\n",
    "            # 'Mother, father and brother were present.'\n",
    "            # If conjuncts are connected by 'and', he predicate is plural\n",
    "            # Exception: 'Every man, woman and child aprticipates in the tournament.'\n",
    "            if 'and' in list(child.text.lower() for child in list(list(subj[0].children)+list(subj[-2].children))):\n",
    "                subj_agr = 'pl'\n",
    "                left_subj_children = list(child.text.lower() for child in list(subj[0].lefts))\n",
    "                left_pred_children = list(child.text.lower() for child in list(pred.lefts))\n",
    "                is_uppercase = True\n",
    "                all_gerund = True\n",
    "                for sub in subj:\n",
    "                    if sub.tag_ != 'NNP':\n",
    "                        is_uppercase = False\n",
    "                    if sub.tag_ != 'VBG':\n",
    "                        all_gerund = False\n",
    "                #Don't check: 'Playing football and enjoying it + is a good thing | are different things)'\n",
    "                if all_gerund:\n",
    "                    continue\n",
    "                #Don't check: 'Jones and Sons is a respectable company.'\n",
    "                if is_uppercase:\n",
    "                    continue\n",
    "                # Don't check: 'There is Tom and Mary as a perfect example.'\n",
    "                if pred.text.lower() in ['is', 'are'] and 'there' in left_pred_children:\n",
    "                    continue\n",
    "                if 'every' in left_subj_children or 'each' in left_subj_children:\n",
    "                    subj_agr = 'sg'\n",
    "                else:\n",
    "                    subj_agr = 'pl'\n",
    "            # 'Mother, father or brother comes to pick up the kid.'\n",
    "            # If conjuncts are connected by 'or', verb agrees with the last one\n",
    "            elif any(a in list(child.text.lower() for child in list(list(subj[0].children)+list(subj[-2].children))) for a in ['or', 'nor']):\n",
    "                if subj[-1].tag_ in ['NNS', 'NNPS'] or subj[-1].text.lower() in plur_only:\n",
    "                    subj_agr = 'pl'\n",
    "                elif subj[-1].tag_ in ['NN', 'NNP', 'VBG'] or subj[-1].text.lower() in sing_only:\n",
    "                    subj_agr = 'sg'\n",
    "\n",
    "        if pred.tag_ == 'VBZ':\n",
    "            pred_agr = 'sg'\n",
    "        elif pred.tag_ == 'VBP':\n",
    "            pred_agr = 'pl'\n",
    "        elif pred.lemma_ == 'be':\n",
    "            if pred.text == 'was':\n",
    "                pred_agr = 'sg'\n",
    "            elif pred.text == 'were':\n",
    "                pred_agr = 'pl'\n",
    "\n",
    "        if subj_agr != pred_agr and subj_agr and pred_agr:\n",
    "            res += [pair]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7YywB0bWnJyp"
   },
   "outputs": [],
   "source": [
    "def search(directory = 'test', only_errors=True): #If used with excel files, should be only used with similar format files, otherwise additional info can be messy\n",
    "    final = []\n",
    "    text_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(directory) for f in filenames if os.path.splitext(f)[1] == '.txt']\n",
    "    excel_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(directory) for f in filenames if os.path.splitext(f)[1] == '.xlsx']\n",
    "\n",
    "    if text_files: #If there is at least one txt file, we assume that the format is default\n",
    "        if only_errors:\n",
    "            final.append((\"Sentence\", \"Errors\", \"Filename\"))\n",
    "        else:\n",
    "            final.append((\"Sentence\", \"Errors\", \"Filename\", \"Has Error\"))\n",
    "\n",
    "    for file in text_files:\n",
    "        text = open_file(file)\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sent in sentences:\n",
    "            sent = re.sub(r'<.*?>', '', sent)\n",
    "            sent = re.sub(r' +', ' ', sent)\n",
    "            doc = nlp(sent)\n",
    "            ps = find_pred_subj(doc)\n",
    "            # ps looks like [(predicate1, [subject1, subject2]), (predicate2, [subject1])]\n",
    "            er = errors(ps)\n",
    "#             er = [error for error in er if not subjunctive(sent, error[0])]\n",
    "            \n",
    "            if only_errors:\n",
    "                if er:\n",
    "                    final.append((sent,\n",
    "                                  \",\\n\".join([\"{} {}\".format(*er[x]) for x in range(len(er))]), # Wrong subjs and preds,\n",
    "                                 file))\n",
    "            else:\n",
    "                final.append((sent,\n",
    "                              \", \".join([\"({} {})\".format(*er[x]) for x in range(len(er))]),\n",
    "                              file,\n",
    "                              int(len(er) != 0)))  #Whether or not errors were found in this sentence\n",
    "\n",
    "    for file in excel_files:\n",
    "        df = pandas.read_excel(file, keep_default_na=False)\n",
    "        if not final:\n",
    "            additional_columns = tuple([x for x in df.columns if x != \"Sentence\"])\n",
    "            if only_errors:\n",
    "                final.append((\"Sentence\", \"Errors\", \"Filename\") + additional_columns)\n",
    "            else:\n",
    "                final.append((\"Sentence\", \"Errors\", \"Filename\", \"Has Error\") + additional_columns)\n",
    "\n",
    "        for row_num in range(df.shape[0]):\n",
    "            sent = df[\"Sentence\"].iat[row_num]\n",
    "            sent = re.sub(r'<.*?>', '', sent)\n",
    "            sent = re.sub(r' +', ' ', sent)\n",
    "            doc = nlp(sent)\n",
    "            ps = find_pred_subj(doc)\n",
    "            # ps looks like [(predicate1, [subject1, subject2]), (predicate2, [subject1])]\n",
    "            er = errors(ps)\n",
    "#             er = [error for error in er if not subjunctive(sent, error[0])]\n",
    "            \n",
    "            if only_errors:\n",
    "                if er:\n",
    "                    final.append((sent,\n",
    "                                  \",\\n\".join([\"{} {}\".format(*er[x]) for x in range(len(er))]), # Wrong subjs and preds,\n",
    "                                 file,\n",
    "                                 *[df[column].iat[row_num] for column in additional_columns]))\n",
    "            else:\n",
    "                final.append((sent,\n",
    "                              \", \".join([\"({} {})\".format(*er[x]) for x in range(len(er))]),\n",
    "                              file,\n",
    "                              int(len(er) != 0), #Whether or not errors were found\n",
    "                              *[df[column].iat[row_num] for column in additional_columns]))\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7w64supcnJys"
   },
   "outputs": [],
   "source": [
    "def subjunctive(sent, pred):\n",
    "    adj_prt = {'advisable', 'best', 'crucial', 'desirable', 'vital',\n",
    "               'essential', 'imperative', 'important', 'moved',\n",
    "               'necessary', 'prohibited', 'unthinkable', 'urgent',\n",
    "               'required', 'stipulated', 'requested', 'recommended',\n",
    "               'advised', 'proposed', 'adamant', 'anxious', \n",
    "               'determined', 'eager', 'insistent', 'keen'}\n",
    "    \n",
    "    clause_is_subjunctive = False\n",
    "    that_list = []\n",
    "\n",
    "    if 'that' in sent.lower():\n",
    "        doc = nlp(sent)\n",
    "        that_list = [token for token in doc if token.text.lower() == 'that']\n",
    "        for that in that_list: \n",
    "            # VERB/NOUN + that + CONJUNCTIVE\n",
    "            if (that.head.i == pred.i \\\n",
    "                and pred.dep_ == 'ccomp' \\\n",
    "                and pred.head.pos_ in ['VERB', 'NOUN'] \\\n",
    "                and pred.head.lemma_ in {'advise', 'ask', 'command', 'demand', 'desire',\n",
    "                                         'insist', 'move', 'order', 'prefer', 'propose',\n",
    "                                         'recommend', 'request', 'stipulate', 'suggest', 'urge',\n",
    "                                         'motion', 'order', 'preference', 'proposal', 'recommendation',\n",
    "                                         'request', 'stipulation', 'suggestion'}):\n",
    "                clause_is_subjunctive = True\n",
    "\n",
    "            # SUBJECT + [be] ADJ/PRTC + that + CONJUNCTIVE\n",
    "            elif (that.head.i == pred.i and pred.dep_ == 'ccomp'):\n",
    "                if (pred.head.lemma_ == 'be' and \\\n",
    "                    any(a in adj_prt for a in list(b.text.lower() for b in pred.head.children))) \\\n",
    "                    or (pred.head.tag_ in ['VBN', 'JJ'] and \\\n",
    "                        pred.head.text in adj_prt):\n",
    "                        clause_is_subjunctive = True\n",
    "\n",
    "    return clause_is_subjunctive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4QFskfGnJyv",
    "outputId": "f5745206-7a79-416f-d7af-3611417a9a5c"
   },
   "outputs": [],
   "source": [
    "ambiguous = {'bison', 'cod', 'deer', 'fish', 'moose', 'boar', 'salmon', 'sheep',\n",
    "            'shrimp', 'swine', 'trout', 'buffalo', 'grouse', 'elk', 'fruit', 'reindeer',\n",
    "            'offspring', 'pike',\n",
    "            'statistics', 'politics', 'mechanics', 'economics',            \n",
    "            'government', 'data', 'police', 'team', 'jury', 'family',\n",
    "            'half', 'class', 'majority', 'part', 'percent', '%', 'cent', 'lot', 'group'}\n",
    "\n",
    "sing_only = {'each', 'either', 'neither', 'one', 'nobody',\n",
    "            'nothing', 'anyone', 'anybody', 'anything', 'someone', \n",
    "            'somebody', 'something', 'everyone', 'everybody', 'everything', \n",
    "             'this', 'one', 'other', 'measles'}\n",
    "\n",
    "plur_only = {'several', 'few', 'many', 'both', 'these', 'those'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yulikdaniel/.local/lib/python3.8/site-packages/torch/autocast_mode.py:141: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 19s, sys: 165 ms, total: 2min 19s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = search('test/evaluate/medium', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16015/371994445.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwriteln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"itog.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_16015/1298828837.py\u001b[0m in \u001b[0;36mwriteln\u001b[0;34m(errors, filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mworksheet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworkbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_worksheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#Fill in the column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mworksheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "writeln(f, \"itog.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(*args):\n",
    "    return search(os.path.join(*args), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(*args, write_errs=None, print_wrong=False): # Function to test the model on a directory with a correct.txt file and a wrong.txt file\n",
    "    output = run_model(*args)\n",
    "    \n",
    "    if write_errs is not None:\n",
    "        writeln(output, write_errs)\n",
    "    \n",
    "    false_pos = 0\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "    for x in output[1:]:\n",
    "        if x[2] == os.path.join(*args, \"correct.txt\"):\n",
    "            if x[3] == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                if print_wrong:\n",
    "                    print(\"False positive:\", x)\n",
    "                false_pos += 1\n",
    "        else:\n",
    "            if x[3] == 0:\n",
    "                if print_wrong:\n",
    "                    print(\"False negative:\", x)\n",
    "                false_neg += 1\n",
    "            else:\n",
    "                true_pos += 1\n",
    "    precision = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "    recall = true_pos / max(1, true_pos + false_neg)\n",
    "    print(\"Precision = {}\\nRecall = {}\\n\".format(round(precision, 2), round(recall, 2)))\n",
    "\n",
    "def evaluate(level, **kwargs): # level should be equal to \"easy\", \"medium\" or \"insane\"\n",
    "    print(level.capitalize(), \"tests:\")\n",
    "    test_model(\"test\", \"evaluate\", level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.0\n",
      "Recall = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(\"test/test_wa\", print_wrong=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium tests:\n",
      "Precision = 0.95\n",
      "Recall = 0.55\n",
      "\n",
      "CPU times: user 2min 40s, sys: 347 ms, total: 2min 40s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVUa4oOMnJyz"
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uoCviYynJy6",
    "outputId": "22c45de3-8918-4794-cbbf-e424fc63af56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The cardsharps\" is a 16th century masterpiece by Caravaggio\n",
      "[(is, [cardsharps])]\n",
      "0\t\"\t\t\tPUNCT\t\t\t``\t\tpunct\t\t4\n",
      "1\tThe\t\t\tDET\tWORK_OF_ART\t\tDT\t\tdet\t\t2\n",
      "2\tcardsharps\t\t\tNOUN\tWORK_OF_ART\t\tNNS\t\tnsubj\t\t4\n",
      "3\t\"\t\t\tPUNCT\t\t\t''\t\tpunct\t\t4\n",
      "4\tis\t\t\tAUX\t\t\tVBZ\t\tROOT\t\t4\n",
      "5\ta\t\t\tDET\t\t\tDT\t\tdet\t\t8\n",
      "6\t16th\t\t\tADJ\tDATE\t\tJJ\t\tamod\t\t7\n",
      "7\tcentury\t\t\tNOUN\tDATE\t\tNN\t\tcompound\t\t8\n",
      "8\tmasterpiece\t\t\tNOUN\t\t\tNN\t\tattr\t\t4\n",
      "9\tby\t\t\tADP\t\t\tIN\t\tprep\t\t8\n",
      "10\tCaravaggio\t\t\tPROPN\tPERSON\t\tNNP\t\tpobj\t\t9\n"
     ]
    }
   ],
   "source": [
    "s = ['\"The cardsharps\" is a 16th century masterpiece by Caravaggio']\n",
    "for ss in s:\n",
    "    doc = nlp(ss)\n",
    "    print(doc)\n",
    "    print(find_pred_subj(doc))\n",
    "    for token in doc:\n",
    "        print(\"%s\\t%s\\t\\t\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (token.i, token.text, token.pos_, \n",
    "                                                        token.ent_type_, token.tag_, token.dep_, token.head.i))\n",
    "    \n",
    "#print(dir(doc[0]))\n",
    "#print(' '.join(list(ch.text for ch in doc[3].children)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(sent):\n",
    "    sent = re.sub(r'<.*?>', '', sent)\n",
    "    sent = re.sub(r' +', ' ', sent)\n",
    "    doc = nlp(sent)\n",
    "    ps = find_pred_subj(doc)\n",
    "    er = errors(ps)\n",
    "    print(ps)\n",
    "    for token in doc:\n",
    "        print(\"%s\\t%s\\t\\t\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (token.i, token.text, token.pos_, \n",
    "                                                        token.ent_type_, token.tag_, token.dep_, token.head.i))\n",
    "    print(er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miss [I]\n",
      "is []\n",
      "[(miss, [I]), (is, [])]\n",
      "0\t\"\t\t\tPUNCT\t\t\t``\t\tpunct\t\t5\n",
      "1\tI\t\t\tPRON\tWORK_OF_ART\t\tPRP\t\tnsubj\t\t2\n",
      "2\tmiss\t\t\tVERB\tWORK_OF_ART\t\tVBP\t\tccomp\t\t5\n",
      "3\tapples\t\t\tNOUN\tWORK_OF_ART\t\tNNS\t\tdobj\t\t2\n",
      "4\t\"\t\t\tPUNCT\t\t\t''\t\tpunct\t\t5\n",
      "5\tis\t\t\tAUX\t\t\tVBZ\t\tROOT\t\t5\n",
      "6\tmy\t\t\tPRON\t\t\tPRP$\t\tposs\t\t7\n",
      "7\tmovie\t\t\tNOUN\t\t\tNN\t\tattr\t\t5\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "check('\"I miss apples\" is my movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "77-IWInUnJzE",
    "outputId": "dc39df0d-099f-4506-c8b0-f0f7e68525ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BsXLrsQonJzH"
   },
   "outputs": [],
   "source": [
    "st = sent_tokenize(\"Apples is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adwisor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
