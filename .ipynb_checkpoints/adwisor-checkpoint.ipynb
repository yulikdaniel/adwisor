{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6gICSGYpnJyI"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_transformers\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import re\n",
    "import xlsxwriter\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tDDcGUD6nJyV"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6LZrxF85nJyY"
   },
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "akthzbfhnJya"
   },
   "outputs": [],
   "source": [
    "def writeln(errors, filename):\n",
    "    workbook = xlsxwriter.Workbook(filename)\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    #Fill in the column names\n",
    "    for x in range(len(errors[0])):\n",
    "        worksheet.write(0, x, errors[0][x])\n",
    "\n",
    "    col = 0\n",
    "    for row_num in range(1, len(errors)):\n",
    "        error = errors[row_num]\n",
    "        for x in range(len(error)):\n",
    "            worksheet.write(row_num, col + x, error[x])\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kBOmgczUnJyc"
   },
   "outputs": [],
   "source": [
    "def find_subj(pred):\n",
    "    # print(pred, [child.text for child in pred.lefts]) use pred.lefts to find quotes\n",
    "    # simple cases\n",
    "    subjects = []\n",
    "    subjects = [child for child in list(pred.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "\n",
    "    # handling 'there is' and 'there are' cases\n",
    "    if pred.lemma_ == 'be' and 'there' in list(i.text.lower() for i in pred.children):\n",
    "        subjects += [child for child in list(pred.children) if child.dep_ == 'attr']\n",
    "\n",
    "    # if predicate is an auxiliary, we want to take subjects of its head\n",
    "    if pred.dep_.startswith('aux'):\n",
    "        subjects += [child for child in list(pred.head.children) if child.dep_.startswith(('nsubj', 'csubj'))]\n",
    "    \n",
    "    # handling conjuncts: multiple subjects as in 'Mother and father are key figures in a child's life'.\n",
    "    add_subj = []\n",
    "    for subject in subjects:\n",
    "        add_subj += list(subject.conjuncts)\n",
    "\n",
    "    cur_pred = pred\n",
    "    while len(subjects) == 0 and cur_pred.dep_ == \"conj\":\n",
    "        cur_pred = cur_pred.head\n",
    "        subjects = find_subj(cur_pred)\n",
    "\n",
    "    subjects += add_subj\n",
    "\n",
    "    # the subjects' order may be different from sentence order, so we arrange it right\n",
    "    subjects.sort(key=lambda subj: subj.i)\n",
    "\n",
    "    if len(subjects) == 1 and subjects[0].text.lower() in ['who', 'that', 'which'] and pred.dep_ == 'relcl':\n",
    "        subjects = [pred.head]\n",
    "\n",
    "    return subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x4mj0yyanJyg"
   },
   "outputs": [],
   "source": [
    "def find_pred_subj(doc):\n",
    "    pred_sub = list()\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['AUX', 'VERB']:\n",
    "            # negation: cases like \"He doesn't scare me\"   \n",
    "            aux = None\n",
    "            if [ch for ch in list(token.lefts) if ch.dep_ == 'neg']:\n",
    "                children = list(token.children)\n",
    "                for ch in children:\n",
    "                    if ch.dep_ == 'aux' and not aux:\n",
    "                        aux = ch\n",
    "                if aux:\n",
    "                    pred_sub += [(aux, find_subj(aux))]\n",
    "\n",
    "\n",
    "            # if the predicate is analytical like 'I have done',\n",
    "            # spacy rightfully considers the participle to be the root,\n",
    "            # but we need grammatical info, so we will consider aux the root\n",
    "            if not aux: # for when negation is expressed with 'never' etc and does not need aux support\n",
    "                if token.tag_ in ['VBN', 'VBG']:\n",
    "                    aux = None\n",
    "                    children = list(token.children)\n",
    "                    for ch in children:\n",
    "                        if ch.dep_ == 'aux' and ch.pos_ in ['VERB', 'AUX'] and ch.tag_ != 'VBN':\n",
    "                            aux = ch\n",
    "                        elif not aux and ch.dep_ == 'auxpass' and ch.pos_ in ['VERB', 'AUX']:\n",
    "                            aux = ch\n",
    "                    if aux:\n",
    "                        pred_sub += [(aux, find_subj(aux))]\n",
    "\n",
    "                # all other cases\n",
    "                elif token.dep_ in ['ROOT', 'ccomp', 'xcomp', 'acl', 'relcl']:\n",
    "                    pred_sub += [(token, find_subj(token))]\n",
    "\n",
    "                # conjuncts: when there are multiple predicates connected by conjunction\n",
    "                elif token.dep_ == 'conj' and token.head.dep_ in ['ROOT', 'ccomp', 'xcomp', 'acl', 'relcl']:\n",
    "                    pred_sub += [(token, find_subj(token))]\n",
    "                \n",
    "    return pred_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "OIP9NwitnJyl"
   },
   "outputs": [],
   "source": [
    "def errors(ps):\n",
    "    res = []\n",
    "    for pair in ps:\n",
    "        subj_agr, pred_agr = None, None #what each must agree with, variables must coincide in the end\n",
    "        pred = pair[0]\n",
    "        subj = pair[1]\n",
    "        if len(subj) == 1:\n",
    "            subject = subj[0]\n",
    "            s = subject.text.lower()\n",
    "            subject_left_children = subject.lefts\n",
    "            subject_is_numeral = False\n",
    "            for child in subject_left_children:\n",
    "                if child.pos_ == 'NUM':\n",
    "                    subject_is_numeral = True\n",
    "                    break\n",
    "            if subject_is_numeral:\n",
    "                continue\n",
    "            if s not in ambiguous:\n",
    "                children = list(ch for ch in subject.children)\n",
    "                children_text = list(ch.text.lower() for ch in children)\n",
    "                \n",
    "                #singular only pronouns\n",
    "                if s in sing_only or subject.ent_type_ == 'ORG':\n",
    "                    subj_agr = 'sg'\n",
    "                    \n",
    "                # either singular or plural pronouns\n",
    "                # if they have an 'of N, N, N...', after them we will require check if verb agrees with the last noun\n",
    "                elif (s in {'some', 'any', 'none', 'all', 'most'} \\\n",
    "                      and 'of' in children_text):\n",
    "                    of = [ch for ch in children if ch.text.lower() == 'of'][0]\n",
    "                    noun = [ch for ch in of.children if ch.pos_ == 'NOUN']\n",
    "                    if noun:\n",
    "                        noun = noun[0]\n",
    "                        while [ch for ch in noun.children if (ch.dep_ == 'conj' and ch.pos_ == 'NOUN')]:\n",
    "                            noun = [ch for ch in noun.children if (ch.dep_ == 'conj' and ch.pos_ == 'NOUN')][-1]\n",
    "                        if noun.tag_ in ['NNS', 'NNPS'] or noun.text.lower() in plur_only:\n",
    "                            subj_agr = 'pl'\n",
    "                        elif noun.tag_ in ['NN', 'NNP'] or noun.text.lower() in sing_only:\n",
    "                            subj_agr = 'sg'\n",
    "                    \n",
    "                # plural only pronouns\n",
    "                elif s in plur_only and not children:\n",
    "                    subj_agr = 'pl'\n",
    "                    \n",
    "                elif s in {'i', 'we', 'you', 'they'}:\n",
    "                    subj_agr = 'pl'\n",
    "                elif s in {'he', 'she', 'it'}:\n",
    "                    subj_agr = 'sg'\n",
    "                    \n",
    "                elif s == 'number':\n",
    "                    if 'a' in children_text and 'of' in children_text:\n",
    "                        subj_agr = 'pl'\n",
    "                    else: subj_agr = 'sg'\n",
    "                    \n",
    "                # predicates in non-head clauses with 'who', 'that' agree with noun in head clause\n",
    "                elif s in ['who', 'that', 'which']:\n",
    "                    if pred.dep_ == 'relcl': \n",
    "                        print(\"This is probably a bug!\")\n",
    "                        #why relcl? we can only be sure about this tag that it is the case we're looking for.\n",
    "                        #other possible predicate tags include '...comp', but these also apply\n",
    "                        #in cases like \"I asked the boys who was the winner\",\n",
    "                        #which, although with incorrect word order,\n",
    "                        #are still clearly present in Russian essays\n",
    "                        #and will be parsed by spacy as 'ccomp'\n",
    "                        head = pred.head\n",
    "                        if not head.conjuncts:\n",
    "                            if head.tag_ in ['NNS', 'NNPS'] or head.text.lower() in plur_only:\n",
    "                                subj_agr = 'pl'\n",
    "                            elif head.tag_ in ['NN', 'NNP'] or head.text.lower() in sing_only:\n",
    "                                subj_agr = 'sg'\n",
    "                        else:\n",
    "                            conjuncts = list(head.conjuncts)+[head]\n",
    "                            for conjunct in conjuncts:\n",
    "                                if 'and' in list(child.text.lower() for child in conjunct.children):\n",
    "                                    subj_agr = 'pl' \n",
    "                        print(subj_agr)\n",
    "                elif subject.tag_ in ['NNS', 'NNPS']:\n",
    "                    subj_agr = 'pl'\n",
    "                elif subject.tag_ in ['NN', 'NNP', 'VBG']:\n",
    "                    subj_agr = 'sg'\n",
    "                if subject.ent_type_ == 'LOC' or subject.ent_type_ == 'GPE':\n",
    "                    if subject.tag_ in ['NNS', 'NNPS']:\n",
    "                        continue\n",
    "                    else:\n",
    "                        subj_arg = 'sg'\n",
    "                    \n",
    "        elif len(subj) > 1:\n",
    "            # 'Mother, father and brother were present.'\n",
    "            # If conjuncts are connected by 'and', he predicate is plural\n",
    "            # Exception: 'Every man, woman and child aprticipates in the tournament.'\n",
    "            if 'and' in list(child.text.lower() for child in list(list(subj[0].children)+list(subj[-2].children))):\n",
    "                subj_agr = 'pl'\n",
    "                left_subj_children = list(child.text.lower() for child in list(subj[0].lefts))\n",
    "                left_pred_children = list(child.text.lower() for child in list(pred.lefts))\n",
    "                is_uppercase = True\n",
    "                all_gerund = True\n",
    "                for sub in subj:\n",
    "                    if sub.tag_ != 'NNP':\n",
    "                        is_uppercase = False\n",
    "                    if sub.tag_ != 'VBG':\n",
    "                        all_gerund = False\n",
    "                #Don't check: 'Playing football and enjoying it + is a good thing | are different things)'\n",
    "                if all_gerund:\n",
    "                    continue\n",
    "                #Don't check: 'Jones and Sons is a respectable company.'\n",
    "                if is_uppercase:\n",
    "                    continue\n",
    "                # Don't check: 'There is Tom and Mary as a perfect example.'\n",
    "                if pred.text.lower() in ['is', 'are'] and 'there' in left_pred_children:\n",
    "                    continue\n",
    "                if 'every' in left_subj_children or 'each' in left_subj_children:\n",
    "                    subj_agr = 'sg'\n",
    "                else:\n",
    "                    subj_agr = 'pl'\n",
    "            # 'Mother, father or brother comes to pick up the kid.'\n",
    "            # If conjuncts are connected by 'or', verb agrees with the last one\n",
    "            elif any(a in list(child.text.lower() for child in list(list(subj[0].children)+list(subj[-2].children))) for a in ['or', 'nor']):\n",
    "                if subj[-1].tag_ in ['NNS', 'NNPS'] or subj[-1].text.lower() in plur_only:\n",
    "                    subj_agr = 'pl'\n",
    "                elif subj[-1].tag_ in ['NN', 'NNP', 'VBG'] or subj[-1].text.lower() in sing_only:\n",
    "                    subj_agr = 'sg'\n",
    "\n",
    "        if pred.tag_ == 'VBZ':\n",
    "            pred_agr = 'sg'\n",
    "        elif pred.tag_ == 'VBP':\n",
    "            pred_agr = 'pl'\n",
    "        elif pred.lemma == 'be':\n",
    "            if pred.text == 'was':\n",
    "                pred_agr = 'sg'\n",
    "            elif pred.text == 'were':\n",
    "                pred_agr = 'pl'\n",
    "\n",
    "        if subj_agr != pred_agr and subj_agr and pred_agr:\n",
    "            res += [pair]\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "7YywB0bWnJyp"
   },
   "outputs": [],
   "source": [
    "def search(directory = 'test', only_errors=True): #If used with excel files, should be only used with similar format files, otherwise additional info can be messy\n",
    "    final = []\n",
    "    text_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(directory) for f in filenames if os.path.splitext(f)[1] == '.txt']\n",
    "    excel_files = [os.path.join(dp, f) for dp, dn, filenames in os.walk(directory) for f in filenames if os.path.splitext(f)[1] == '.xlsx']\n",
    "\n",
    "    if text_files: #If there is at least one txt file, we assume that the format is default\n",
    "        if only_errors:\n",
    "            final.append((\"Sentence\", \"Errors\", \"Filename\"))\n",
    "        else:\n",
    "            final.append((\"Sentence\", \"Errors\", \"Filename\", \"Has Error\"))\n",
    "\n",
    "    for file in text_files:\n",
    "        text = open_file(file)\n",
    "        sentences = sent_tokenize(text)\n",
    "        for sent in sentences:\n",
    "            sent = re.sub(r'<.*?>', '', sent)\n",
    "            sent = re.sub(r' +', ' ', sent)\n",
    "            doc = nlp(sent)\n",
    "            ps = find_pred_subj(doc)\n",
    "            print(sent)\n",
    "            # ps looks like [(predicate1, [subject1, subject2]), (predicate2, [subject1])]\n",
    "            er = errors(ps)\n",
    "#             er = [error for error in er if not subjunctive(sent, error[0])]\n",
    "            \n",
    "            if only_errors:\n",
    "                if er:\n",
    "                    final.append((sent,\n",
    "                                  \",\\n\".join([\"{} {}\".format(*er[x]) for x in range(len(er))]), # Wrong subjs and preds,\n",
    "                                 file))\n",
    "            else:\n",
    "                final.append((sent,\n",
    "                              \", \".join([\"({} {})\".format(*er[x]) for x in range(len(er))]),\n",
    "                              file,\n",
    "                              int(len(er) != 0)))  #Whether or not errors were found in this sentence\n",
    "\n",
    "    for file in excel_files:\n",
    "        df = pandas.read_excel(file, keep_default_na=False)\n",
    "        if not final:\n",
    "            additional_columns = tuple([x for x in df.columns if x != \"Sentence\"])\n",
    "            if only_errors:\n",
    "                final.append((\"Sentence\", \"Errors\", \"Filename\") + additional_columns)\n",
    "            else:\n",
    "                final.append((\"Sentence\", \"Errors\", \"Filename\", \"Has Error\") + additional_columns)\n",
    "\n",
    "        for row_num in range(df.shape[0]):\n",
    "            sent = df[\"Sentence\"].iat[row_num]\n",
    "            sent = re.sub(r'<.*?>', '', sent)\n",
    "            sent = re.sub(r' +', ' ', sent)\n",
    "            doc = nlp(sent)\n",
    "            ps = find_pred_subj(doc)\n",
    "            # ps looks like [(predicate1, [subject1, subject2]), (predicate2, [subject1])]\n",
    "            er = errors(ps)\n",
    "#             er = [error for error in er if not subjunctive(sent, error[0])]\n",
    "            \n",
    "            if only_errors:\n",
    "                if er:\n",
    "                    final.append((sent,\n",
    "                                  \",\\n\".join([\"{} {}\".format(*er[x]) for x in range(len(er))]), # Wrong subjs and preds,\n",
    "                                 file,\n",
    "                                 *[df[column].iat[row_num] for column in additional_columns]))\n",
    "            else:\n",
    "                final.append((sent,\n",
    "                              \", \".join([\"({} {})\".format(*er[x]) for x in range(len(er))]),\n",
    "                              file,\n",
    "                              int(len(er) != 0), #Whether or not errors were found\n",
    "                              *[df[column].iat[row_num] for column in additional_columns]))\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7w64supcnJys"
   },
   "outputs": [],
   "source": [
    "def subjunctive(sent, pred):\n",
    "    adj_prt = {'advisable', 'best', 'crucial', 'desirable', 'vital',\n",
    "               'essential', 'imperative', 'important', 'moved',\n",
    "               'necessary', 'prohibited', 'unthinkable', 'urgent',\n",
    "               'required', 'stipulated', 'requested', 'recommended',\n",
    "               'advised', 'proposed', 'adamant', 'anxious', \n",
    "               'determined', 'eager', 'insistent', 'keen'}\n",
    "    \n",
    "    clause_is_subjunctive = False\n",
    "    that_list = []\n",
    "\n",
    "    if 'that' in sent.lower():\n",
    "        doc = nlp(sent)\n",
    "        that_list = [token for token in doc if token.text.lower() == 'that']\n",
    "        for that in that_list: \n",
    "            # VERB/NOUN + that + CONJUNCTIVE\n",
    "            if (that.head.i == pred.i \\\n",
    "                and pred.dep_ == 'ccomp' \\\n",
    "                and pred.head.pos_ in ['VERB', 'NOUN'] \\\n",
    "                and pred.head.lemma_ in {'advise', 'ask', 'command', 'demand', 'desire',\n",
    "                                         'insist', 'move', 'order', 'prefer', 'propose',\n",
    "                                         'recommend', 'request', 'stipulate', 'suggest', 'urge',\n",
    "                                         'motion', 'order', 'preference', 'proposal', 'recommendation',\n",
    "                                         'request', 'stipulation', 'suggestion'}):\n",
    "                clause_is_subjunctive = True\n",
    "\n",
    "            # SUBJECT + [be] ADJ/PRTC + that + CONJUNCTIVE\n",
    "            elif (that.head.i == pred.i and pred.dep_ == 'ccomp'):\n",
    "                if (pred.head.lemma_ == 'be' and \\\n",
    "                    any(a in adj_prt for a in list(b.text.lower() for b in pred.head.children))) \\\n",
    "                    or (pred.head.tag_ in ['VBN', 'JJ'] and \\\n",
    "                        pred.head.text in adj_prt):\n",
    "                        clause_is_subjunctive = True\n",
    "\n",
    "    return clause_is_subjunctive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4QFskfGnJyv",
    "outputId": "f5745206-7a79-416f-d7af-3611417a9a5c"
   },
   "outputs": [],
   "source": [
    "ambiguous = {'bison', 'cod', 'deer', 'fish', 'moose', 'boar', 'salmon', 'sheep',\n",
    "            'shrimp', 'swine', 'trout', 'buffalo', 'grouse', 'elk', 'fruit', 'reindeer',\n",
    "            'offspring', 'pike',\n",
    "            'statistics', 'politics', 'mechanics', 'economics',            \n",
    "            'government', 'data', 'police', 'team', 'jury', 'family',\n",
    "            'half', 'class', 'majority', 'part', 'percent', '%', 'cent', 'lot', 'group'}\n",
    "\n",
    "sing_only = {'each', 'either', 'neither', 'one', 'nobody',\n",
    "            'nothing', 'anyone', 'anybody', 'anything', 'someone', \n",
    "            'somebody', 'something', 'everyone', 'everybody', 'everything', \n",
    "             'this', 'one', 'other'}\n",
    "\n",
    "plur_only = {'several', 'few', 'many', 'both', 'these', 'those'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This chart shows us the data about the unemployment in a few world areas.\n",
      "The data is divided into the information about 2014 and 2015 unemployment levels.\n",
      "It is clearly seen that there are two main trends.\n",
      "Firstly, the level of unemployment did not change at some areas such as North Africa and South Asia where that level remained stable.\n",
      "Secondly, it is shown that that level decreased at some regions.\n",
      "Talking about details, we can see that the level of unemployment decreased rapidly in the two regions.\n",
      "It went from 11 to 9,6 points in the Middle East, and it also went down from 6,8 to 5,8 points in the Latin America.\n",
      "However, there is an unusual trend.\n",
      "In contrast to the previous examples, the EU's level of unemployment rose by the 2015 from 7,5 to 7,9 points.\n",
      "In general, it is shown that there were different trends at the different areas.\n",
      "However, the worldwide indicator did not change at all.\n",
      "Nowadays there are a lot of disputes over pros and cons of illegal music and music copying.\n",
      "Some people believe that all the content on the Internet should be free and available to everyone, while their opponents argue that illegal copying can be claimed as a theft or at least as a kind of crime.\n",
      "Actually, the opinion that pirate copying should be punished is really widespread around the world.\n",
      "People who support that position usually provide three main arguments.\n",
      "Firstly, illegal copying affects the amount of money that people of art earn every day.\n",
      "It reduces their income so people become less motivated to work at the sphere of arts.\n",
      "Secondly, it is bad for the governments economies because the illegal spreading of that content cannot be taxed.\n",
      "Thirdly, these illegal actions can undermine the moral side of modern societies.\n",
      "It means that nowadays some people start to think that something \"illegal\" can sometimes be a good (or normal) thing.\n",
      "It happens because people face illegal spreading almost every day what makes it normal for them, therefore the governments want to stop pirate copying immediately.\n",
      "However, there is an opposite point of view on this issue.\n",
      "Some people think that every person in the world should be able to find all the content he or her needs for free.\n",
      "Advocates of this opinion say that some people just cannot afford buying films and music while they need it to increase their levels of culture and education.\n",
      "In my opinion, every work should be rewarded so it is not fair that film producers, actors and musicians lose some part of their money because of pirate copies.\n",
      "However, I believe that all the educational movies and pieces of art should be free as it is important for the level of culture worldwide.\n",
      "The bar charts illustrate how the level of unemployment changed in different parts of the world across 2014 and 2015.\n",
      "It is important to note, that in some of the regions the rates did not change at all.\n",
      "At the same time, in Middle East and Latin America the level of unemployment decreased and in the EU, on the contrary, increased.\n",
      "While in Middle East the rates dropped from 11 per cent to 9.6. in Latin America the level of unemployment reached the point of 5.8 per cent in 2015 in comparison to 6.8% in 2014.\n",
      "What is also important is that the highest rates were noted in North America.\n",
      "The level of unemployment in South Africa is lower than in the whole world in average.\n",
      "Also in Latin America the rates reached lower point in comparison to the worldwide level in 2015, although in 2014 they were higher.\n",
      "All in all, the worldwide level of unemployment remained stable during this period.\n",
      "It is thought, that the creators of films and music lose a lot of money due to the fact that these files usually are spread on the internet illegally, that is why some people are sure that such things should be obliged.\n",
      "This is probably a bug!\n",
      "On the one hand, musicians and film producers fail to receive the money that they should actually get after their creations were released.\n",
      "What is more, such things are usually posted in poor quality, thus people who listen to or watch them cannot realise, what great job was done to create it.\n",
      "They will not be able to face the effects the producers use in their films, understand, how wonderful is the sound in the songs.\n",
      "Moreover, if we speak about official buying in iTunes, for instance, it also helps the owners of such platforms to pet money.\n",
      "What is more, due to copies more people in the whole world will know about the stuff and its producers.\n",
      "This is the way for them to become even more popular.\n",
      "On the other hand, after watching the film or listening to music on the internet a person may decide to go to the cinema or buy the song or even the whole album if he or she liked it indeed.\n",
      "The fact is that if someone knows that really good things were produced and is sure that things really go this way, the creators will receive even much money.\n",
      "All in all, it seems that the producers lose too much money because of copying as it is thought.\n",
      "The spread of their creativity may make them very famous and rich indeed.\n",
      "If a person is popular with things he or she does that will enable to produce more new extraordinary films and songs.\n",
      "This is probably a bug!\n",
      "Finally, it seems that there is no serious reason for trying to prevent copying.\n",
      "The bar chart represents the information about unemployment rate in selected world regions.\n",
      "It is easily seen, that the highest level of unemployment is in the North Africa.\n",
      "The lowest unemployment can be found in South Asia, and in this region we can also see stable level of unemployment.\n",
      "To the other regions the percentage of unemployment is erratic.\n",
      "In EU there is low increase of unemployment, when in the Latin America and Middle East the rate decreased.\n",
      "In general it could be found that the worldwide amount of unemployment has stable rate, If we compare 2014 and 2015.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-814e272408db>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(directory, only_errors)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'<.*?>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' +'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_pred_subj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1012\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         \u001b[0minstall_extensions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/pipeline_component.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTransformerBatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m         \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerListener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlistener\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisteners\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/spacy_transformers/layers/transformer_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     )\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbp_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordpieces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"logger\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mlog_gpu_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"logger\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"after forward\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/layers/pytorchwrapper.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     \u001b[0mYtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_backprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtorch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dYtorch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtorch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArgsKwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/thinc/shims/pytorch.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mixed_precision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         )\n\u001b[0;32m--> 848\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 )\n\u001b[1;32m    521\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    523\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "f = search('test/evaluate/medium', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeln(f, \"itog.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(*args):\n",
    "    return search(os.path.join(*args), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(*args, write_errs=None, print_wrong=False): # Function to test the model on a directory with a correct.txt file and a wrong.txt file\n",
    "    output = run_model(*args)\n",
    "    \n",
    "    if write_errs is not None:\n",
    "        writeln(output, write_errs)\n",
    "    \n",
    "    false_pos = 0\n",
    "    true_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "    for x in output[1:]:\n",
    "        if x[2] == os.path.join(*args, \"correct.txt\"):\n",
    "            if x[3] == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                if print_wrong:\n",
    "                    print(\"False positive:\", x)\n",
    "                false_pos += 1\n",
    "        else:\n",
    "            if x[3] == 0:\n",
    "                if print_wrong:\n",
    "                    print(\"False negative:\", x)\n",
    "                false_neg += 1\n",
    "            else:\n",
    "                true_pos += 1\n",
    "    precision = (true_pos + true_neg) / (true_pos + true_neg + false_pos + false_neg)\n",
    "    recall = true_pos / max(1, true_pos + false_neg)\n",
    "    print(\"Precision = {}\\nRecall = {}\\n\".format(round(precision, 2), round(recall, 2)))\n",
    "\n",
    "def evaluate(level, **kwargs): # level should be equal to \"easy\", \"medium\" or \"insane\"\n",
    "    print(level.capitalize(), \"tests:\")\n",
    "    test_model(\"test\", \"evaluate\", level, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.0\n",
      "Recall = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_model(\"test/test_wa\", print_wrong=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium tests:\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "This is probably a bug!\n",
      "Precision = 0.95\n",
      "Recall = 0.53\n",
      "\n",
      "CPU times: user 2min 23s, sys: 168 ms, total: 2min 24s\n",
      "Wall time: 36.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "evaluate(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVUa4oOMnJyz"
   },
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uoCviYynJy6",
    "outputId": "22c45de3-8918-4794-cbbf-e424fc63af56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is thought that the creators of films and music lose a lot of money due to the fact that these files usually are spread on the internet illegally, that is why some people are sure that such things should be obliged.\n",
      "[(is, [It]), (lose, [creators]), (are, [files]), (is, [that]), (are, [people]), (should, [things])]\n",
      "0\tIt\t\t\tPRON\tit\t\tPRP\t\tnsubjpass\t\t2\n",
      "1\tis\t\t\tAUX\tbe\t\tVBZ\t\tauxpass\t\t2\n",
      "2\tthought\t\t\tVERB\tthink\t\tVBN\t\tccomp\t\t31\n",
      "3\tthat\t\t\tSCONJ\tthat\t\tIN\t\tmark\t\t10\n",
      "4\tthe\t\t\tDET\tthe\t\tDT\t\tdet\t\t5\n",
      "5\tcreators\t\t\tNOUN\tcreator\t\tNNS\t\tnsubj\t\t10\n",
      "6\tof\t\t\tADP\tof\t\tIN\t\tprep\t\t5\n",
      "7\tfilms\t\t\tNOUN\tfilm\t\tNNS\t\tpobj\t\t6\n",
      "8\tand\t\t\tCCONJ\tand\t\tCC\t\tcc\t\t7\n",
      "9\tmusic\t\t\tNOUN\tmusic\t\tNN\t\tconj\t\t7\n",
      "10\tlose\t\t\tVERB\tlose\t\tVBP\t\tccomp\t\t2\n",
      "11\ta\t\t\tDET\ta\t\tDT\t\tdet\t\t12\n",
      "12\tlot\t\t\tNOUN\tlot\t\tNN\t\tdobj\t\t10\n",
      "13\tof\t\t\tADP\tof\t\tIN\t\tprep\t\t12\n",
      "14\tmoney\t\t\tNOUN\tmoney\t\tNN\t\tpobj\t\t13\n",
      "15\tdue\t\t\tADP\tdue\t\tIN\t\tprep\t\t10\n",
      "16\tto\t\t\tADP\tto\t\tIN\t\tpcomp\t\t15\n",
      "17\tthe\t\t\tDET\tthe\t\tDT\t\tdet\t\t18\n",
      "18\tfact\t\t\tNOUN\tfact\t\tNN\t\tpobj\t\t15\n",
      "19\tthat\t\t\tSCONJ\tthat\t\tIN\t\tmark\t\t24\n",
      "20\tthese\t\t\tDET\tthese\t\tDT\t\tdet\t\t21\n",
      "21\tfiles\t\t\tNOUN\tfile\t\tNNS\t\tnsubjpass\t\t24\n",
      "22\tusually\t\t\tADV\tusually\t\tRB\t\tadvmod\t\t24\n",
      "23\tare\t\t\tAUX\tbe\t\tVBP\t\tauxpass\t\t24\n",
      "24\tspread\t\t\tVERB\tspread\t\tVBN\t\tacl\t\t18\n",
      "25\ton\t\t\tADP\ton\t\tIN\t\tprep\t\t24\n",
      "26\tthe\t\t\tDET\tthe\t\tDT\t\tdet\t\t27\n",
      "27\tinternet\t\t\tNOUN\tinternet\t\tNN\t\tpobj\t\t25\n",
      "28\tillegally\t\t\tADV\tillegally\t\tRB\t\tadvmod\t\t24\n",
      "29\t,\t\t\tPUNCT\t,\t\t,\t\tpunct\t\t31\n",
      "30\tthat\t\t\tPRON\tthat\t\tDT\t\tnsubj\t\t31\n",
      "31\tis\t\t\tAUX\tbe\t\tVBZ\t\tROOT\t\t31\n",
      "32\twhy\t\t\tSCONJ\twhy\t\tWRB\t\tadvmod\t\t35\n",
      "33\tsome\t\t\tDET\tsome\t\tDT\t\tdet\t\t34\n",
      "34\tpeople\t\t\tNOUN\tpeople\t\tNNS\t\tnsubj\t\t35\n",
      "35\tare\t\t\tAUX\tbe\t\tVBP\t\tccomp\t\t31\n",
      "36\tsure\t\t\tADJ\tsure\t\tJJ\t\tacomp\t\t35\n",
      "37\tthat\t\t\tSCONJ\tthat\t\tIN\t\tmark\t\t42\n",
      "38\tsuch\t\t\tADJ\tsuch\t\tJJ\t\tamod\t\t39\n",
      "39\tthings\t\t\tNOUN\tthing\t\tNNS\t\tnsubjpass\t\t42\n",
      "40\tshould\t\t\tAUX\tshould\t\tMD\t\taux\t\t42\n",
      "41\tbe\t\t\tAUX\tbe\t\tVB\t\tauxpass\t\t42\n",
      "42\tobliged\t\t\tVERB\toblige\t\tVBN\t\tccomp\t\t36\n",
      "43\t.\t\t\tPUNCT\t.\t\t.\t\tpunct\t\t31\n",
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = [\"It is thought that the creators of films and music lose a lot of money due to the fact that these files usually are spread on the internet illegally, that is why some people are sure that such things should be obliged.\"]\n",
    "\n",
    "for ss in s:\n",
    "    doc = nlp(ss)\n",
    "    print(doc)\n",
    "    print(find_pred_subj(doc))\n",
    "    \n",
    "    \n",
    "for token in doc:\n",
    "    print(\"%s\\t%s\\t\\t\\t%s\\t%s\\t\\t%s\\t\\t%s\\t\\t%s\" % (token.i, token.text, token.pos_, token.lemma_, token.tag_, token.dep_, token.head.i))\n",
    "    \n",
    "print(dir(doc[0]))\n",
    "print(' '.join(list(ch.text for ch in doc[3].children)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "77-IWInUnJzE",
    "outputId": "dc39df0d-099f-4506-c8b0-f0f7e68525ea",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nominal subject'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"nsubj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "BsXLrsQonJzH"
   },
   "outputs": [],
   "source": [
    "st = sent_tokenize(\"Apples is good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "adwisor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
